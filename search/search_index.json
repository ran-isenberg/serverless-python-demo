{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Homepage","text":""},{"location":"#the-pragmatic-python-serverless-developer","title":"The Pragmatic Python Serverless Developer","text":""},{"location":"#the-problem","title":"The Problem","text":"<p>Starting a Serverless service can be overwhelming. You need to figure out many questions and challenges that have nothing to do with your business domain:</p> <ul> <li>How to deploy to the cloud? What IAC framework do you choose?</li> <li>How to write a SaaS-oriented CI/CD pipeline? What does it need to contain?</li> <li>How do you write a Lambda function?</li> <li>How do you handle observability, logging, tracing, metrics?</li> <li>How do you handle testing?</li> </ul>"},{"location":"#the-solution","title":"The Solution","text":"<p>This project provides a serverless service which is based an opinionated approach to Python project setup, testing, profiling, deployments, and operations. Learn about many open source tools, including Powertools for AWS Lambda\u2014a toolkit that can help you implement serverless best practices and increase developer velocity.</p>"},{"location":"#serverless-template","title":"Serverless Template","text":"<p>The service we present here started from the AWS Lambda Handler Cookbook, a serverless service template project.</p> <p>The template project provides a working, deployable, open source-based, serverless service template with an AWS Lambda function and AWS CDK Python code with all the best practices and a complete CI/CD pipeline.</p> <p>You can start your own service in three clicks.</p>"},{"location":"#the-products-service","title":"The Products service","text":"<p>Features\u00b6 <ul> <li>Python Serverless service with a recommended file structure.</li> <li>CDK infrastructure with infrastructure tests and security tests.</li> <li>Both synchronous and asynchronous resources</li> <li>CI/CD pipelines based on Github actions that deploys to AWS with python linters, complexity checks and style formatters.</li> <li>CI/CD pipeline deploys to dev/staging and production environment with different gates between each environment</li> <li>Makefile for simple developer experience.</li> <li>The AWS Lambda handler embodies Serverless best practices and has all the bells and whistles for a proper production ready handler.</li> <li>AWS Lambda handler uses AWS Lambda Powertools: logger, tracer, metrics, event handler, validation, batch</li> <li>AWS Lambda handler three layer architecture: handler layer, logic layer and data access layer (integration)</li> <li>Cognito user pool with a test user. All E2E tests automatically login with the test user and use a JWT id token</li> <li>Idempotent API</li> <li>Authentication protected API</li> <li>REST API protected by WAF with four AWS managed rules in production deployment</li> <li>CloudWatch dashboards - High level and low level including CloudWatch alarms</li> <li>Unit, infrastructure, security, integration and end to end tests.</li> </ul> <p>While the code examples are written in Python, the principles are valid to any supported AWS Lambda handler programming language.</p>"},{"location":"#monitoring-design","title":"Monitoring Design","text":"<p>Async Testing Design\u00b6 <p>License\u00b6 <p>This library is licensed under the MIT License. See the LICENSE file.</p> <p>Maintained by Ran Isenberg and Heitor Lessa</p>"},{"location":"decision_log/","title":"Decision log","text":""},{"location":"decision_log/#decision-log","title":"Decision log","text":"<p>This file is meant to capture project-level decisions that were made in this project and why. There are often no obvious correct answers, and we must decide with multiple options.</p>"},{"location":"decision_log/#2023-10-30","title":"2023-10-30","text":"<p>Added Amazon Cognito user pool. While it is not connected as external identity provider, or provides registration, it is a good start for any service. We build a test user in the CDK, and set its password, upload it to secrets manager and use it in the E2E tests to trigger the protected API. Its a good start to have a protected API during development prior to productization.</p>"},{"location":"decision_log/#2023-10-17","title":"2023-10-17","text":"<p>Added serverless monitoring with CloudWatch dashboards: logs, metrics, custom metrics and alarms.</p> <p>We use 'cdk-monitoring-constructs' open-source because it simplifies the dashboard creation and it is very simple to use.</p> <p>Based on concepts and examples described here</p>"},{"location":"decision_log/#2023-08-17","title":"2023-08-17","text":""},{"location":"decision_log/#project-structure","title":"Project Structure","text":""},{"location":"decision_log/#what","title":"What","text":"<p>We chose an opinionated project structure with an infrastructure folder (CDK-based), a tests folder, and a service folder containing business domain Lambda function code with a makefile to automate developer actions.</p>"},{"location":"decision_log/#why","title":"Why","text":"<p>This structure has proven its worth in production for us. However, there's no right or wrong; other structures might make sense to you.</p> <p>You can read more about it here.</p>"},{"location":"decision_log/#cdk","title":"CDK","text":""},{"location":"decision_log/#what_1","title":"What","text":"<p>We chose AWS CDK as the IaC of choice.</p>"},{"location":"decision_log/#why_1","title":"Why","text":"<ul> <li>There are many IaC options. We chose CDK since we enjoy working with it and have a very positive experience with it and with the experience of defining resources in code instead of YAML files.</li> <li>Choose what fits your organization best: AWS SAM, Serverless, Terraform, Pulumi, etc.</li> </ul>"},{"location":"decision_log/#cdk-constructs-structure","title":"CDK Constructs Structure","text":""},{"location":"decision_log/#what_2","title":"What","text":"<p>We defined a stack that creates two constructs: one for the crud API and one for the stream processing. The CRUD API also makes the database construct.</p>"},{"location":"decision_log/#why_2","title":"Why","text":"<p>We chose a business domain-driven Constructs approach. Each domain gets its construct, with the DB being the exception as an \"inner\" construct.</p> <p>We don't think there's a right or wrong approach to picking resources into constructs as long as it makes sense to you and you can find help and configurations easily.</p> <p>However, choosing a business domain-driven approach to selecting which resources belong together in a construct makes sense the most.</p> <p>Finding resources and understanding their connections is more accessible by looking at the service architecture diagram. It's also easier to share design patterns across teams in organizations that require the same architecture.</p> <p>You can read more about it with a similar example here</p>"},{"location":"decision_log/#cdk-best-practices","title":"CDK Best Practices","text":""},{"location":"decision_log/#what_3","title":"What","text":"<ul> <li>Stack per developer per branch, stack name is different</li> <li>Shared resources are built on the stack and passed as parameters to the constructs init functions.</li> <li>Lambda roles define inline policy definitions instead of using CDK's built-in functions</li> </ul>"},{"location":"decision_log/#why_3","title":"Why","text":"<ul> <li>Stack per developer per branch - we wanted multiple developers to share a dev account and work in parallel on the same stack. The CI/CD main pipeline has its unique name to remove any chance of conflicts.</li> <li>Shared resources - made sense to build once and pass internal resources such as Lambda layers.</li> <li>Lambda roles define inline policy definitions instead of using CDK's built-in functions - CDK's built-in 'grant' function is less privileged and provides more resources than required. They also reduce visibility and abstract actual permissions too much.</li> </ul> <p>You can read more about it here</p>"},{"location":"decision_log/#lambda-layers-usage","title":"Lambda Layers Usage","text":""},{"location":"decision_log/#what_4","title":"What","text":"<p>We use a Lambda layer that all our Lambda functions use.</p>"},{"location":"decision_log/#why_4","title":"Why","text":"<p>We use it as a deployment optimization since all our functions require mostly (or the same) dependencies.</p> <p>It comprises all '[tool.poetry.dependencies]' in the 'pyproject. toml' file.</p> <p>You can read more about creating Lambda layers here and best practices here.</p>"},{"location":"decision_log/#build-folder","title":".build folder","text":""},{"location":"decision_log/#what_5","title":"What","text":"<p>We use as build stage as part of 'make deploy' to copy the Lambda contents from 'product' folder to '.build'.</p>"},{"location":"decision_log/#why_5","title":"Why","text":"<p>You must supply an asset folder when building a Lambda layer/lambda function with CDK. It removes the top folder and takes the contents.</p> <p>If we were to supply the 'product' folder as the root folder, we would get import issues when invoking the function since the imports in our lambda function contain 'product.x.y' same as it resides on GitHub.</p> <p>To solve this issue, we have a build step that it runs when you run 'make deploy'; it copies the 'product' folder from the root level to a new root level folder, the '.build.'</p> <p>This way, when CDK takes the lambda contents from this new top level, it also takes the 'product' top folder and the imports remain valid.</p>"},{"location":"decision_log/#lambda-architecture-layers","title":"Lambda architecture layers","text":""},{"location":"decision_log/#what_6","title":"What","text":"<p>Under product, you have several folders: one per domain.</p> <p>Each domain: crud and stream processor, have different layers.</p> <p>We have different architectural layers: handler -&gt; domain logic -&gt; data access layer.</p> <p>Each layer has folders for Pydantic schema classes and utilities.</p>"},{"location":"decision_log/#why_6","title":"Why","text":"<p>This is an opinionated structure backed by AWS best practices to separate handler code from domain logic.</p> <p>You can read more about it here.</p>"},{"location":"decision_log/#testing-methodology","title":"Testing methodology","text":""},{"location":"decision_log/#what_7","title":"What","text":"<p>We have unit tests, infrastructure tests, integration tests, and end-to-end tests.</p>"},{"location":"decision_log/#why_7","title":"Why","text":"<p>Each test type has its usage:</p> <ul> <li>Unit tests check small functions and mostly schema validations.</li> <li>Infrastructure tests are run before deployment; they check that critical resources exist and were not deleted from the CloudFormation template (CDK output) by mistake or bug.</li> <li>Integration tests occur after deployment and generate a mocked event, call the function handler in the IDE and allow debugging the functions with breakpoints. We call real AWs services and can choose what to mock to simulate failures and what resources to call directly.</li> <li>E2E tests - trigger the AWS deployed resources.</li> </ul> <p>You can read about testing methodology and how to test serverless in this blog series</p>"},{"location":"decision_log/#pydantic-usage","title":"Pydantic Usage","text":""},{"location":"decision_log/#what_8","title":"What","text":"<p>We are using pydantic for environment variables parsing, schema validation of input/output, and more.</p>"},{"location":"decision_log/#why_8","title":"Why","text":"<p>Pydantic is a class leader parsing and validation Python library.</p> <p>Input validation is a critical security aspect that each application must implement.</p> <p>Read more about input validation in Lambda here.</p> <p>Read more about why you should care about environment variables parsing here.</p>"},{"location":"monitoring/","title":"Monitoring","text":""},{"location":"monitoring/#key-concepts","title":"Key Concepts","text":"<p>Utilizing AWS CloudWatch dashboards enables centralized monitoring of API Gateway, Lambda functions, and DynamoDB, providing real-time insights into their performance and operational health.</p> <p>By aggregating metrics, logs, and alarms, CloudWatch facilitates swift issue diagnosis and analysis across your serverless applications. Additionally, setting up alarms ensures immediate alerts during anomalous activities, enabling proactive issue mitigation.</p>"},{"location":"monitoring/#service-architecture","title":"Service Architecture","text":"<p>The goal is to monitor the service API gateway, Lambda function, and DynamoDB tables and ensure everything is in order.</p> <p>In addition, we want to visualize service KPI metrics.</p>"},{"location":"monitoring/#monitoring-dashboards","title":"Monitoring Dashboards","text":"<p>We will define two dashboards:</p> <ul> <li>High level</li> <li>Low level</li> </ul> <p>Each dashboard has its usage and tailors different personas' usage.</p>"},{"location":"monitoring/#high-level-dashboard","title":"High Level Dashboard","text":"<p>We have a high level dashboard per module: CRUD and Streaming.</p> <p>This dashboard is designed to be an executive overview of the service.</p> <p>Total API gateway metrics provide information on the performance and error rate of the service.</p> <p>KPI metrics are included in the bottom part as well.</p> <p>Personas that use this dashboard: SRE, developers, and product teams (KPIs)</p> <p></p>"},{"location":"monitoring/#low-level-dashboard","title":"Low Level Dashboard","text":"<p>It is aimed at a deep dive into all the service's resources. Requires an understanding of the service architecture and its moving parts.</p> <p>The dashboard provides the Lambda function's metrics for latency, errors, throttles, provisioned concurrency, and total invocations.</p> <p>In addition, a CloudWatch logs widget shows only 'error' logs from the Lambda function.</p> <p>As for DynamoDB tables, we have the primary database and the idempotency table for usage, operation latency, errors, and throttles.</p> <p>Personas that use this dashboard: developers, SREs.</p> <p>We have two dashboards : one for CRUD module and the other for the streaming module.</p>"},{"location":"monitoring/#crud","title":"CRUD","text":""},{"location":"monitoring/#streaming","title":"Streaming","text":""},{"location":"monitoring/#alarms","title":"Alarms","text":"<p>Having visibility and information is one thing, but being proactive and knowing beforehand that a significant error is looming is another. A CloudWatch</p> <p>Alarm is an automated notification tool within AWS CloudWatch that triggers alerts based on user-defined thresholds, enabling users to identify and</p> <p>respond to operational issues, breaches, or anomalies in AWS resources by monitoring specified metrics over a designated period.</p> <p>In this dashboard, you will find an example of two types of alarms:</p> <ul> <li>Alarm for performance threshold monitoring</li> <li>Alarm for error rate threshold monitoring</li> </ul> <p>For latency-related issues, we define the following alarm:</p> <p></p> <p>For P90, P50 metrics, follow this explanation.</p> <p>For internal server errors rate, we define the following alarm: </p>"},{"location":"monitoring/#actions","title":"Actions","text":"<p>Alarms are of no use unless they have an action. We have configured the alarms to send an SNS notification to a new SNS topic. From there, you can connect any subscription - HTTPS/SMS/Email, etc. to notify your teams with the alarm details.</p>"},{"location":"monitoring/#cdk-reference","title":"CDK Reference","text":"<p>We use the open-source cdk-monitoring-constructs.</p> <p>You can view find the monitoring CDK construct here.</p>"},{"location":"monitoring/#further-reading","title":"Further Reading","text":"<p>If you wish to learn more about this concept and go over details on the CDK code, check out my blog post.</p>"},{"location":"opensource/","title":"Open Source","text":""},{"location":"opensource/#open-source","title":"Open Source","text":"<p>This page lists all open source libraries this project currently uses.</p> <ul> <li>aws-lambda-powertools</li> <li>serverless template</li> <li>mypy</li> <li>pydantic</li> <li>cachetools</li> <li>aws-lambda-env-modeler</li> <li>cdk-nag</li> <li>cdk-monitoring-constructs</li> <li>radon</li> <li>xenon</li> <li>pre-commit</li> <li>ruff</li> <li>mkdocs-material</li> <li>AWS CDK</li> </ul>"},{"location":"pipeline/","title":"CI/CD Pipeline","text":""},{"location":"pipeline/#getting-started","title":"Getting Started","text":"<p>The GitHub CI/CD pipeline includes the following steps.</p> <p>The pipelines uses environment secrets (under the defined environment 'dev', 'staging' and 'production') for code coverage and for the role to deploy to AWS.</p> <p>When you clone this repository be sure to define the environments in your repo settings and create a secret per environment:</p> <ul> <li>AWS_ROLE - to role to assume for your GitHub worker as defined here</li> </ul>"},{"location":"pipeline/#makefile-commands","title":"Makefile Commands","text":"<p>All steps can be run locally using the makefile. See details below:</p> <ul> <li>Create Python environment ands install dev dependencie with <code>make dev</code></li> <li>Run pre-commit checks as defined in <code>.pre-commit-config.yaml</code></li> <li>Lint and format and sort imports with ruff (similar to flake8/yapf/isort) - run <code>make format</code> in the IDE</li> <li>Static type check with mypy - run <code>make lint</code> in the IDE</li> <li>Verify that Python imports are sorted according to standard - run <code>make sort</code> in the IDE</li> <li>Python complexity checks: radon and xenon  - run <code>make complex</code> in the IDE</li> <li>Unit tests. Run <code>make unit</code> to run unit tests in the IDE</li> <li>Infrastructure test. Run <code>make infra-tests</code> to run the CDK infrastructure tests in the IDE</li> <li>Code coverage by codecov.io</li> <li>Deploy CDK - run <code>make deploy</code> in the IDE, will also run security tests based on cdk_nag</li> <li>E2E tests  - run <code>make e2e</code> in the IDE</li> <li>Code coverage tests  - run <code>make coverage-tests</code> in the IDE after CDK dep</li> </ul>"},{"location":"pipeline/#other-capabilities","title":"Other Capabilities","text":"<ul> <li>Automatic Python dependencies update with Dependabot</li> <li>Easy to use makefile allows to run locally all commands in the GitHub actions</li> <li>Run local docs server, prior to push in pipeline - run <code>make docs</code>  in the IDE</li> <li>Prepare PR, run all checks with one command - run <code>make pr</code> in the IDE</li> </ul>"},{"location":"pipeline/#environments-pipelines","title":"Environments &amp; Pipelines","text":"<p>All GitHub workflows are stored under <code>.github/workflows</code> folder.</p> <p>The two most important ones are <code>pr-serverless-service</code>  and <code>main-serverless-service</code>.</p>"},{"location":"pipeline/#pr-serverless-service","title":"pr-serverless-service","text":"<p><code>pr-serverless-service</code> runs for every pull request you open. It expects you defined a GitHub environment by the name <code>dev</code> and that it includes a secret by the name of <code>AWS_ROLE</code>.</p> <p>It includes two jobs: 'quality_standards' and 'tests' where a failure in 'quality_standards' does not trigger 'tests'. Both jobs MUST pass in order to to be able to merge.</p> <p>'quality_standards' includes all linters, pre-commit checks and units tests and 'tests' deploys the service to AWS, runs code coverage checks, security checks and E2E tests. Stack is destroyed at the end. Stack has a 'dev' prefix as part of its name.</p> <p>Once merged, <code>main-serverless-service</code> will run.</p>"},{"location":"pipeline/#main-serverless-service","title":"main-serverless-service","text":"<p><code>main-serverless-service</code> runs for every MERGED pull request that runs on the main branch. It expects you defined a GitHub environments by the name <code>staging</code> and <code>production</code> and that both includes a secret by the name of <code>AWS_ROLE</code>.</p> <p>It includes three jobs: 'staging', 'production' and 'publish_github_pages'.</p> <p>'staging' does not run any of the 'quality_standards' checks, since they were already checked before the code was merged. It runs just coverage tests and E2E tests. Stack is not deleted. Stack has a 'staging' prefix as part of its name. Any failure in staging will stop the pipeline and production environment will not get updated with the new code.</p> <p>'production' does not run any of the 'quality_standards' checks, since they were already checked before the code was merged. It does not run any test at the moment. Stack is not deleted. Stack has a 'production' prefix as part of its name.</p>"},{"location":"api/product_models/","title":"Product models","text":""},{"location":"api/product_models/#product-models","title":"Product models","text":"<p>::: product.models.products.product</p>"},{"location":"api/product_models/#validators","title":"Validators","text":"<p>::: product.models.products.validators</p>"},{"location":"api/stream_processor/","title":"Stream Processor","text":""},{"location":"api/stream_processor/#product-notification","title":"Product notification","text":""},{"location":"api/stream_processor/#lambda-handlers","title":"Lambda Handlers","text":"<p>Process stream is connected to Amazon DynamoDB Stream that polls product changes in the product table.</p> <p>We convert them into <code>ProductChangeNotification</code> model depending on the DynamoDB Stream Event Name (e.g., <code>INSERT</code> -&gt; <code>ADDED</code>).</p> <p>::: product.stream_processor.handlers.process_stream</p>"},{"location":"api/stream_processor/#domain-logic","title":"Domain logic","text":"<p>Domain logic to notify product changes, e.g., <code>ADDED</code>, <code>REMOVED</code>, <code>UPDATED</code>.</p> <p>::: product.stream_processor.domain_logic.product_notification</p>"},{"location":"api/stream_processor/#integrations","title":"Integrations","text":"<p>These are integrations with external services. As of now, we only use one integration to send events, by default <code>Amazon EventBridge</code>.</p> <p>NOTE: We could make a single Event Handler. For now, we're using one event handler closely aligned with the model we want to convert into event for type safety.</p> <p>::: product.stream_processor.integrations.events.event_handler</p>"},{"location":"api/stream_processor/#events","title":"Events","text":"<p>::: product.stream_processor.integrations.events.models.input</p> <p>::: product.stream_processor.integrations.events.models.output</p>"},{"location":"api/stream_processor/#providers","title":"Providers","text":"<p>::: product.stream_processor.integrations.events.providers.eventbridge</p>"},{"location":"api/stream_processor/#interfaces","title":"Interfaces","text":"<p>::: product.stream_processor.integrations.events.base</p>"},{"location":"api/stream_processor/#utility-functions","title":"Utility functions","text":"<p>::: product.stream_processor.integrations.events.functions ::: product.stream_processor.integrations.events.constants</p>"},{"location":"api/stream_processor/#exceptions","title":"Exceptions","text":"<p>::: product.stream_processor.integrations.events.exceptions</p>"}]}